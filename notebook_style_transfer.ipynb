{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQflL7D_QVdb"
   },
   "source": [
    "# Neural style transfer\n",
    "\n",
    "---\n",
    "> **This notebook is quite compute intensive: it might be better to run it on Google Colab (or on Kaggle). <br>\n",
    "> Depending on your recent Google Colab usage, Kaggle may be faster.**\n",
    "---\n",
    "\n",
    "As seen in our previous courses, Machine Learning models are usually used to solve industrial problems such as classifiyng satellite images or extracting features of an image. Apart from these \"real-life\" industrial problems, some research works have less \"practical\" and direct purposes but are more artistic by often introducing artistic generative methods and models. \n",
    "**Style transfer** is a technique aiming at generating a new image by merging a content image with a style image. For example, we can merge a picture of ISAE-SUPAERO's building with the style of some paintings or visual works :\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1lb37kiTRzqu5QhjJMkZyF4XUSLsI3o6p\" width=\"80%\" style=\"background-color:white\">\n",
    "</center>\n",
    "\n",
    "\n",
    "This notebook is divided into two main parts. The first section [Style transfer baseline](#section1) presents the basics of neural style transfer, and reproduces the results of the article [1]. The second section [Real-time style transfer](#section2), that relies on a part of the article [4], improves the results of the first section by speeding up the generation process. A final short section [Going further](#section3) presents some improvements in visual neural style transfer, and also some adaptations of style transfer in other domains, especially in Music Information Retrieval.\n",
    "\n",
    "In the following, we will call:\n",
    "- The **content image**: the original image on which we want to transfer the style.\n",
    "- The **style image**: the original image that contains the required style.\n",
    "- The **combined image**: the output image that combines the content image with the style image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to download data and set up your session. If you are on Kaggle, check that the \"Internet\" toggle is switched on. The `%load` magic command does not seem to work on Colab, so simply copy-paste from the .py file if needed, or run a `!cat file` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2ksMEmTQvcs",
    "outputId": "fc533f43-ed0a-4db3-b161-b6b14bb2f212"
   },
   "outputs": [],
   "source": [
    "# Download data (size ~250 Mo)\n",
    "!wget https://nextcloud.isae.fr/index.php/s/DwP7K9HpmNKZaR2/download -O data.zip\n",
    "    \n",
    "!unzip -q data.zip\n",
    "!rm data.zip\n",
    "\n",
    "!mkdir train\n",
    "!unzip -q data/coco.zip -d train\n",
    "!unzip -q data/saved_models.zip \n",
    "!unzip -q data/images.zip\n",
    "!unzip -q data/solutions.zip\n",
    "!unzip -q data/results.zip\n",
    "!mv data/utils.py ./utils.py\n",
    "!rm -r data\n",
    "\n",
    "!mkdir models\n",
    "!rm -rf sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1BXXgE8QVde",
    "outputId": "5788dac9-7659-4aae-c543-7430aba3c2e2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython.display import display, HTML, Audio\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P22pwY6oGCGE"
   },
   "source": [
    "<a class=\"anchor\" id=\"section1\"></a>\n",
    "## Style transfer baseline \n",
    "\n",
    "This section is based on [Gatys & al. (2015), A neural algorithm of artistic style](https://arxiv.org/pdf/1508.06576.pdf), the main paper that tackled the issue of style transfer. This algorithm is an unsupervised method that manages at generating one combined image from a content image and a style image.\n",
    "\n",
    "The style transfer method described in this paper relies on three main ideas that can differ from other machine learning problems:\n",
    "- We do not train a model itself. The optimized parameters are **not** the layers' parameters, but the **input** of the model.\n",
    "- We compute a custom loss that takes into account the content and the style.\n",
    "- The loss is computed using abstractions of the images, and not the images themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Egzc1OQCJG4"
   },
   "source": [
    "### Overall process\n",
    "The overall process in presented in this figure :\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1WKQ6SgrAdZ8lz_Atq8a_gfywH9mHYA1Y\" style=\"border: solid 1pt #cccccc\" width=\"75%\"><br>\n",
    "Figure 1: Overall style transfer process presented in Gatys & al. (2015)\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "We first choose a content image $I_{content}$, a style image $I_{style}$, and we initalize a combined image $I_{comb}$ randomly. Then, we repeat this process:\n",
    "- (1) Feed the combined image in a pre-trained VGG-19.\n",
    "- (2) Using the original content and style images, compute the loss of the output of the VGG-19, which is the weighted sum of a content and a style loss.\n",
    "- (3) Backpropagate to the combined image and **only** update the pixels of the combined image (i.e. the VGG-19's parameters are not updated).\n",
    "\n",
    "We then return the combined image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfS65tS5CJG5"
   },
   "source": [
    "The main remaining questions are thus : \n",
    "- *What are the inputs of the loss function ?*\n",
    "- *How can we define a content loss and a style loss ?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1QARAgBYEMJ"
   },
   "source": [
    "### Data loader\n",
    "\n",
    "We first load our data: the style image and the content image. Moreover, the pre-trained VGG-19 model used below has been initially trained on the ImageNet dataset. Thus, it is usual to normalize our input images using the ImageNet mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiR0YcSWSP3K"
   },
   "outputs": [],
   "source": [
    "img_size = 512 \n",
    "prep = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), # turn to BGR\n",
    "        transforms.Normalize(mean=[0.406, 0.456, 0.485], # subtract imagenet mean\n",
    "                            std=[1,1,1]),\n",
    "        transforms.Lambda(lambda x: x.mul_(255)),\n",
    "    ])\n",
    "\n",
    "post = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "        transforms.Normalize(mean=[-0.406, -0.456, -0.485], # add imagenet mean\n",
    "                            std=[1,1,1]),\n",
    "        transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), # turn to RGB\n",
    "        transforms.Lambda(lambda x: torch.clip(x,0,1)),\n",
    "        transforms.ToPILImage()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAsO9EVDQVdn"
   },
   "outputs": [],
   "source": [
    "style_filename = \"images/style-images/mosaic.jpg\"\n",
    "content_filename = \"images/content-images/isae.jpg\"\n",
    "\n",
    "img_names = [style_filename, content_filename]\n",
    "imgs = [Image.open(name) for name in img_names]\n",
    "imgs_torch = [prep(img).unsqueeze(0).to(device) for img in imgs]\n",
    "style_img, content_img = imgs_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "bjulPia4Sk6y",
    "outputId": "22221403-e79e-4a40-d078-a97dc408766e"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "utils.show_img_gatys(style_img, ax=ax1, title=\"Style\")\n",
    "utils.show_img_gatys(content_img, ax=ax2, title=\"Content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aSOBuLZQVdh"
   },
   "source": [
    "<a class=\"anchor\" id=\"subsection-model\"></a>\n",
    "### Model \n",
    "\n",
    "In order to get an abstraction of an image, the transfer style method uses the features found in a pre-trained VGG-19 network. We thus use the following network, that is to say the full VGG-19 network in which the upper classification dense layers are removed:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1hIY24ERLVEUfOPJDuXX7Dpk1R1TBGmsM\" style=\"border: solid 1pt #cccccc\"/><br>\n",
    "Figure 2: VGG-19 network and feature maps used for content and style losses\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "As described more precisely in the following section [Loss](#subsection-loss), we will use the intermediate outputs of some layers as abstractions of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "a2ed53227b40453b81014b6d35654c14",
      "b4319dea70944bc99fc4e0402199de6d",
      "61bf4e11174040259c2d3827c1e5b49a",
      "e6f37e53503f4050ba1fd4ccc4be2294",
      "a1d0535e7f8b4e49890606f0ab9405da",
      "f0337011b2e947689873747f8586b8b1",
      "2b144d8b56f64be28c385b7e011566d5",
      "49348527438e4cc5af9cd7355851ab92",
      "df13b85061454cfca118216f50e65b8f",
      "6a1dcb3bcc524cf0b24e202a7d62e506",
      "737c53fecd7e4e4cb1d92691cd09c4f4"
     ]
    },
    "id": "5Wjjw3PxQVdh",
    "outputId": "b7e37eb1-709c-44b4-e5ad-c588f2db3ad0"
   },
   "outputs": [],
   "source": [
    "full_vgg19 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSt7tJ_NCJHA"
   },
   "source": [
    "We define a more practical model, so that the output of the model is not only the result after the fifth MaxPooling2D, but also all the required intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aWggT3yQVdi"
   },
   "outputs": [],
   "source": [
    "class VGG19Features(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19Features, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, out_keys):\n",
    "        # arguments\n",
    "        # ---------\n",
    "        # out_keys : list[str]\n",
    "        #    IDs of layers used in the loss function (both style and content)\n",
    "        #\n",
    "        # return\n",
    "        # ------\n",
    "        # list[torch.tensor] \n",
    "        #     List of layers' output, in the same order as out_keys \n",
    "        \n",
    "        out = dict()\n",
    "        out['r11'] = F.relu(self.conv1_1(x))\n",
    "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
    "        out['p1'] = self.pool1(out['r12'])\n",
    "        \n",
    "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
    "        out['p2'] = self.pool2(out['r22'])\n",
    "        \n",
    "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
    "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
    "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
    "        out['p3'] = self.pool3(out['r34'])\n",
    "        \n",
    "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
    "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
    "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
    "        out['p4'] = self.pool4(out['r44'])\n",
    "        \n",
    "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
    "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
    "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
    "        out['p5'] = self.pool5(out['r54'])\n",
    "        \n",
    "        return [out[key] for key in out_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOsZv94WQVdj",
    "outputId": "203cd866-fe86-4154-ed0e-1598fef79d2e"
   },
   "outputs": [],
   "source": [
    "VGG19Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fcmT5UcCJHB"
   },
   "source": [
    "We now set the parameters of the torch pre-trained model to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zofCzL6DQVdk"
   },
   "outputs": [],
   "source": [
    "# Retrieve parameters from the full VGG\n",
    "list_params_original = []\n",
    "for layer in full_vgg19.features:\n",
    "    if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "        params = layer.parameters()\n",
    "        list_params_original.append(torch.nn.utils.parameters_to_vector(params))\n",
    "        \n",
    "# Set params to the new model\n",
    "def init_params(model):\n",
    "    i = 0\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "            torch.nn.utils.vector_to_parameters(list_params_original[i], layer.parameters())\n",
    "            i += 1\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPzn091oCJHC"
   },
   "outputs": [],
   "source": [
    "# Check that our model and the torch model give both the same result\n",
    "mtest = VGG19Features()\n",
    "init_params(mtest)\n",
    "\n",
    "x0 = torch.rand(4, 3, 100, 100)\n",
    "assert torch.equal(full_vgg19.features(x0), mtest(x0, ['p5'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Vm1mm304U48"
   },
   "outputs": [],
   "source": [
    "# Initialization of the VGG model, that will then NEVER be updated\n",
    "vgg19 = VGG19Features()\n",
    "init_params(vgg19)\n",
    "vgg19 = vgg19.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMoQZr1RwrM4"
   },
   "source": [
    "<a class=\"anchor\" id=\"subsection-loss\"></a>\n",
    "### Loss\n",
    "\n",
    "The loss function is the key idea of the paper. We want to quantify these two statements: \"we can still recognize the content of the image\" and \"the image is in the style of the style image\". Thus, the loss function is the sum of two terms, the content loss and the style loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXMxSx7TCJHD"
   },
   "source": [
    "#### Content loss\n",
    "\n",
    "First of all, we want to define a content loss. \n",
    "\n",
    "We can take a simple example: the content image is a gray house, and here are combined images. \n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1DLiBFAQbOb45XbHITbpxOgkBumsglKyA\" style=\"border: solid 1pt #cccccc\" width=\"40%\"/></center>\n",
    "\n",
    "The three combined images are also obviously houses, but if we compare the content image and a combined image pixel per pixel, they will be very different. So a pixel per pixel MSE between the content and the combination won't be relevant.\n",
    "\n",
    "Instead, it would be better to compute the loss between abstractions of the content image and the combined image. We can thus use feature maps resulting from one of the late convolution layers of the VGG-19 presented below. Indeed, given that we use a *pre-trained* model, we hope that the model has learnt to store high-level information in the late feature maps, that is to say, information about the content of the image. Therefore to compare the content of two images, we will compare their two corresponding feature maps. \n",
    "\n",
    "In practice, we will simply compute the MSE between a late feature map with the content image as input, and that same feature map with the combined image as input:\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1PuNIlW1LXC3OgB5yIJOhKuzzOA2TTxGr\" style=\"border: solid 1pt #cccccc\" width=\"80%\"/><br>\n",
    "Figure 3: Content loss\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "where $F^\\ell_{comb}$ (resp. $F^\\ell_{content}$) is the feature map $\\ell$ found in the VGG-19 model, with the combined (resp. content) image as input.\n",
    "\n",
    "The paper suggests to use the feature maps resulting from the convolutional layer `conv4_2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQTw4ZP7CJHE"
   },
   "source": [
    "<font color=\"green\">\n",
    "<hr>\n",
    "    \n",
    "> **Your turn !** <br>\n",
    "> - `loss_fn_content` is the content loss function. <br>\n",
    "> - `content_targets` is a list of tensors of size `len(content_layers)` containing the objects passed to the content loss function during the training process.\n",
    "<hr>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4ehRdc6CJHE"
   },
   "outputs": [],
   "source": [
    "content_layers = ['r42']\n",
    "loss_fn_content = ...\n",
    "content_targets = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat solutions/ex1.py\n",
    "# %load solutions/ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09YW0ssPCJHG"
   },
   "source": [
    "#### Style loss \n",
    "\n",
    "The style loss is less direct. Indeed, the style can be caracterized by the color, the texture, eventual repeated patterns... The paper describes the style as \"features correlations\", that is to say how often two features co-occur. For instance, in the Van Gogh's *Starry Night*, we want to describe the style as: \"there are a lot of blue lines\". We now want to quantify these co-occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohryPt2zCJHG"
   },
   "source": [
    "Let's take a simple example (adapted from [3]). Let's consider a simple convolutional layer with two output channels. The first one detects diagonal lines and the second one detects green objects. We feed the layer with a 4x4 image like that:\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1LlBG9DWK7xdbYz-2xuBKbtEvCdOOVc3Z\" width=\"30%\"/></center>\n",
    "\n",
    "Thus, the content of the feature maps is the following. The diagonal feature map captures the two diagonal lines on the left and the green feature map captures the green objects at the bottom.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1VyLCDgVEN1xgYONoHcu1mqd8RPhl4sUk\" width=\"60%\"/></center>\n",
    "\n",
    "If we compute the [Froebenius inner product](https://en.wikipedia.org/wiki/Frobenius_inner_product) between these two feature maps (i.e. the inner product of the two flattened matrices), we have :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\ 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\ 1 & 1\n",
    "\\end{bmatrix}\n",
    "= 1\n",
    "$$\n",
    "\n",
    "Now, let's take another image as input of the convolutional layer:\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1TeTqdRyv9UgHPVsmmjv2gCrD3rYpD5-i\" width=\"60%\"/></center>\n",
    "\n",
    "We also compute the inner product :\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\ 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\ 1 & 1\n",
    "\\end{bmatrix}\n",
    "= 4\n",
    "$$\n",
    "\n",
    "The second inner product is greater than the first one. We managed to quantify the observation that \"green diagonal lines have a greater contribution in the style of the second image than in the first image\". In other words, we managed to put a single value describing the co-occurrence between these two features \"green\" and \"diagonal\". Now, let's formalize and generalize this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GF4sO8-LCJHH"
   },
   "source": [
    "To describe a style, we want to quantify how often a feature co-occurs with another feature. Thus, from the output of one convolutional layer of depth $K$, we will compute $K^2$ inner products, corresponding to the inner product between each pair of feature maps, and store them in a $K \\times K$ matrix, called [Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix). This Gram matrix is thus the fingerprint of the style of an image. \n",
    "\n",
    "Visually, for a set $F^\\ell$ of feature maps of size $(h, w, K)$ resulting from a convolutional layer: \n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1-XyuRauYVqHX8SpjFmY091k514Lko3n1\" width=\"20%\"/></center>\n",
    "\n",
    "we construct the Gram matrix $G^\\ell$ of size $K \\times K$, in which, for $A_i, A_j$, two features maps of size $h \\times w$ :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\forall(i,j) \\in [1 ... K]^2,~~G_{ij}^\\ell \n",
    "    & = \\langle A_i, A_j \\rangle \\\\\n",
    "    & = \\text{flatten}(A_i) \\times \\text{flatten}(A_j)^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\langle\\cdot,\\cdot\\rangle$ is the Frobenius inner product.\n",
    "\n",
    "Thus, the Gram matrix contains the style information captured by one convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rwVMB7kCJHH"
   },
   "source": [
    "We can then compute multiple Gram matrices from multiple layers within the VGG-19 model. To compare styles of two images, we thus compare the Gram matrices of these layers, by applying a mean-squared error function. \n",
    "To sum up, we first retrieve the Gram matrices of the style image and the Gram matrices of the combined image and we then compute the MSE between these two Gram matrices:\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1EwPOTfaHLtB2lVl1Ni7-CXB_FFkZbzic\" style=\"border: solid 1pt #cccccc\" width=\"90%\"/><br>\n",
    "Figure 4: Style loss\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "And we reiterate this process for each chosen layer. The final style loss is then :\n",
    "$$\n",
    "    \\mathcal{L}_{style} = \\sum_\\ell w_\\ell E_\\ell\n",
    "$$\n",
    "\n",
    "where $w_\\ell$ is an eventual weight and $E_\\ell$, the style loss of one layer:\n",
    "$$\n",
    "    E_\\ell = \\text{MSE}\\left(\\text{Gram}(F^\\ell_{comb}), \\text{Gram}(F^\\ell_{content})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "The paper suggests to use the feature maps $\\ell$ resulting from the convolutional layers `conv1_1`, `conv2_1`, `conv3_1`, `conv4_1` and `conv5_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Efv-tbZnyCnS"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(y):\n",
    "    \"\"\"Computes the Gram matrix of a set of feature maps\"\"\"\n",
    "    (b, ch, h, w) = y.size() # batch, channels, height, width\n",
    "    # Flatten the feature map\n",
    "    features = y.view(b, ch, w * h) \n",
    "    # Transpose the flatten matrix without modifying the batch and channel dimensions\n",
    "    features_t = features.transpose(1, 2) \n",
    "    # Batch-matrix-multiplication and normalization by channels*height*width\n",
    "    gram = features.bmm(features_t) / (ch*h*w)\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2quyMbF8CJHI"
   },
   "source": [
    "<font color=\"green\">\n",
    "<hr>\n",
    "    \n",
    "> **Your turn !** <br>\n",
    "> - `out` is the value of the style loss between a feature map `feature` and its corresponding `target` stored in `style_targets`.  <br> \n",
    "> - `loss_fn_style` is the style loss function. <br>\n",
    "> - `style_targets` is a list of tensors of size `len(style_layers)` containing the objects passed to the style loss function during the training process.\n",
    "<hr>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Efv-tbZnyCnS"
   },
   "outputs": [],
   "source": [
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, feature, target):\n",
    "        out = ...\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Efv-tbZnyCnS"
   },
   "outputs": [],
   "source": [
    "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "loss_fn_style = ...\n",
    "style_targets = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat solutions/ex2.py\n",
    "# %load solutions/ex2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat solutions/ex3.py\n",
    "# %load solutions/ex3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "futTnM4TCJHJ"
   },
   "source": [
    "The final loss if a weighted sum between the style loss and the content loss :\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{total}(I_{comb}) = \\alpha.\\mathcal{L}_{content}(I_{comb}, I_{content}) + \\beta.\\mathcal{L}_{style}(I_{comb}, I_{style})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4veKCwZwy6d"
   },
   "outputs": [],
   "source": [
    "# Style and content weights for the total loss\n",
    "style_weight = 1e5\n",
    "content_weight = 1e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1K4zbQ0MzYO"
   },
   "source": [
    "### Training\n",
    "\n",
    "We can now begin the training process. To recap, we initialize a random combined image. We then feed the VGG-19 with a combined image. After having extracted features to compute the content and the style losses, we backpropagate the gradient through the network **without updating its parameters**. Once the gradient arrives to the input combined image, we now **update the pixels of this combined image**. And we reiterate this process with this new updated combined image as input.\n",
    "\n",
    "Some practical steps before training. \n",
    "- First, just to make it handier, we concatenate the lists to call `model()` only once, instead of twice (to compute the content loss and then the style loss). \n",
    "- We can also `.detach()` the targets (high-level feature map for content, and Gram matrices for style) given that they are fixed values: the gradient does not need to backpropagate through them. Thus, detaching them from the computation graph can save (a small amount) of computation power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CGxGErpCJHL"
   },
   "outputs": [],
   "source": [
    "loss_layers = style_layers + content_layers\n",
    "targets = [target.detach() for target in (style_targets + content_targets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of initializing the first combined image as a random image, we can initialize it by cloning the content image. Thus, the content loss will increase (given that adding the style will twist the actual content), but we will get an acceptable result faster. You can initialize it with noise instead, if you want.\n",
    "- We use the [L-BFGS](https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html) optimizer, as suggested in the baseline material: that is why we must define the required function `closure()`. Note that the params argument is `out_img` - and not something like `model.parameters()` that we usually write in previous courses.\n",
    "\n",
    "On Colab, it takes ~ 5 minutes (and on Kaggle ~ 1 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nf-9sGPyF5S",
    "outputId": "1486bc2f-3bf8-469c-f82c-d09a4b240f24"
   },
   "outputs": [],
   "source": [
    "# Initialize the output image\n",
    "## with noise\n",
    "# out_img = torch.randn_like(content_img).requires_grad_()\n",
    "## with the content image\n",
    "out_img = content_img.clone().requires_grad_()\n",
    "\n",
    "\n",
    "# ===== Hyper-parameters =====\n",
    "max_iter = 500\n",
    "optimizer = torch.optim.LBFGS([out_img])\n",
    "# ============================\n",
    "\n",
    "show_every = 50\n",
    "save_every = 10\n",
    "n_iter = 0\n",
    "list_images = [] # To make the animation\n",
    "\n",
    "utils.print_head()\n",
    "    \n",
    "\n",
    "# Training loop\n",
    "def closure():\n",
    "    global n_iter\n",
    "\n",
    "    # Set gradients of out_img at zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # `out` is a list of tensors, corresponding to each feature maps indexed by loss_layers\n",
    "    out = vgg19(out_img, loss_layers)\n",
    "\n",
    "    # Retrieve style_losses and content_losses\n",
    "    style_losses = []\n",
    "    content_losses = []\n",
    "    for feat_i, feature in enumerate(out):\n",
    "        if feat_i < len(style_layers):\n",
    "            style_losses.append(style_weight * loss_fn_style(feature, targets[feat_i])) \n",
    "        else: \n",
    "            content_losses.append(content_weight * loss_fn_content(feature, targets[feat_i])) \n",
    "\n",
    "    # The total loss is the sum of style and content losses\n",
    "    content_loss = sum(content_losses)\n",
    "    style_loss = sum(style_losses) \n",
    "    loss = content_loss + style_loss\n",
    "    \n",
    "    # Updates the image\n",
    "    loss.backward()\n",
    "    n_iter += 1\n",
    "\n",
    "    # Log\n",
    "    if n_iter % show_every == (show_every-1):\n",
    "        utils.print_log(n_iter, content_loss, style_loss, loss)\n",
    "        # print([f\"{loss_layers[li]} : {l.item()}\" for li, l in enumerate(layer_losses)])\n",
    "\n",
    "    if n_iter % save_every == (save_every-1):\n",
    "        list_images.append(post(out_img.clone().data[0].cpu().squeeze()))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "while n_iter <= max_iter:\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Once the training process has ended, we can retrieve `out_img` (that we optimized in the training loop) to visualize it. Some style images give better results than other images: for instance, Van Gogh's *Starry Night* or Hokusai's *Great Wave* are perceptually less convincing, probably due to more complex features and a less homogeneous texture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "mejk2o9RycMs",
    "outputId": "fae172c1-374b-4ddd-ecbc-37221691771d"
   },
   "outputs": [],
   "source": [
    "utils.show_img_gatys(out_img)\n",
    "plt.gcf().set_size_inches(7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pICYQRII-DL"
   },
   "outputs": [],
   "source": [
    "utils.save_img_gatys(out_img, filename='combined_image_gatys.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQdguWyyB6gD"
   },
   "source": [
    "We can also visualize the evolution of `out_image` during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3q4Mk3j_AlA"
   },
   "outputs": [],
   "source": [
    "anim = utils.make_gif_one_canvas(list_images)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, here is the impact of the relative weighting between content and style. The indicated value is the ratio $\\beta/\\alpha$, i.e. the ratio style weight on content weight. For these images, `out_img` was first initialized with noise.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=17Un-UJmHRbZK7LiXpxViWpDrwJ1HqA5M\" width=\"90%\" style=\"background-color:white\">\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "To conclude this first section, we have managed to merge a content image and a style image to generate this combined image.\n",
    "\n",
    "However, you may notice that the main drawback of this method was that generating *one* image is already time-consuming. For example, it would not be suited for video style transfer, or worse for real-time style transfer.\n",
    "The next section will focus on improving this method to generate images **of the same \n",
    "style** more quickly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvZRvAPFv0Zp"
   },
   "source": [
    "<a class=\"anchor\" id=\"section2\"></a>\n",
    "## Real-time style transfer \n",
    "\n",
    "This section is based on [Johnson & al. (2016), Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155), that improves Gatys's paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvZRvAPFv0Zp"
   },
   "source": [
    "### Overall process\n",
    "\n",
    "The style transfer process relies on two models. A first model aims at **generating combined images** from a content image and is **trained** on a specific style. The second model, similarly as Gatys's paper, is a VGG-16 that aims at **abstracting images**, from which we will retrieve feature maps to compute content and style losses.\n",
    "\n",
    "The overall process is presented in this figure:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1wDsTN9UCY2E9ghf2RhDfooauk3Pwsaca\" style=\"border: solid 1pt #cccccc\" width=\"100%\"><br>\n",
    "Figure 5: Overall style transfer process presented in Johnson & al. (2016)\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "We first choose an image dataset, a style image $I_{style}$, and we initalize the image transformation network randomly. Then, we repeat this process:\n",
    "- (1) Feed an image from the dataset into the transformation network.\n",
    "- (2) This network then produces a combined image $I_{comb}$.\n",
    "- (3) Feed this combined image $I_{comb}$ in a pre-trained VGG-16.\n",
    "- (4) Using the original content image (i.e. the image from the dataset) and the chosen style image, compute the loss of the output of the VGG-16 like in Gatys's paper.\n",
    "- (5) Backpropagate to the transformation network and **only** update its parameters (i.e. the VGG-16 and any images are updated).\n",
    "\n",
    "Thus, the image transformation network will learn to generate images **of the same style** as $I_{style}$. To transfer this style on our images, we simply feed them in this network and get the returned combined image.\n",
    "\n",
    "To recap, the main difference with the Gatys's paper is that we will now **optimize a model**, and not the input of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "Like above, the style image will be normalized with the ImageNet mean and standard deviation. The content images will also be normalized before being fed to the networks during the training process.\n",
    "\n",
    "The model is trained on the [COCO Dataset](https://cocodataset.org/) that gathers images (and annotations, that we won't use). Models provided in the paper are actually trained on the complete dataset (330.000 images). To make it faster, the data downloaded at the beginning contains only 1000 images of the COCO dataset. Though, we manage to get acceptable results only with this small amount of training data.\n",
    "Some completely trained models are provided in `saved_models/` (both provided with the article, and manually trained for this workshop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Hyper-parameters =====\n",
    "image_size = 128\n",
    "dataset = \"train\"\n",
    "lr = 1e-2\n",
    "batch_size = 4\n",
    "style_image = \"images/style-images/mosaic.jpg\"\n",
    "epochs = 10\n",
    "content_weight = 1e5\n",
    "style_weight = 1e10\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(dataset, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batch(batch):\n",
    "    # Normalize using imagenet mean and std\n",
    "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    batch = batch.div_(255.0)\n",
    "    return (batch - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "style = Image.open(style_image).convert('RGB')\n",
    "style = style_transform(style)\n",
    "style = style.repeat(batch_size, 1, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "\n",
    "In the figure above, the first network \"Transform Image Network\" learns to generate combined images from a content image and is trained on an image dataset. It is based on downsampling convolutional layers, residual blocks and upsampling convolutional layers. The network is detailed in the paper's [supplementary material](https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf).\n",
    "\n",
    "\n",
    "The second network is used to extract different levels of abstraction of the combined image in order to compute the content and the style losses. Like in Gatys's paper, a convolutional network is used. However, it specifically uses the features of a VGG-16 model, which is similar to the VGG-19, except three missing convolutional layers.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1pvXn8cjoJm2nPf4vXg4o8z2uSKeJ2wL3\" style=\"border: solid 1pt #cccccc\" width=\"90%\"><br>\n",
    "Figure 6: VGG-16 network and feature maps used for content and style losses\n",
    "</center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = utils.initialize_pretrained_vgg16().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "We use the same loss as above, that is to say the sum of a content loss and a style loss. The content loss remains a MSE between two late feature maps (i.e. two high-level abstractions of the image). The style loss remains the sum $E_\\ell$ on a few layers $\\ell$, where $E_\\ell$ is the MSE between Gram matrices, that represent co-occurrences of features.\n",
    "\n",
    "<font color=\"green\">\n",
    "<hr>\n",
    "    \n",
    "> **Your turn !** (Similar to above...)<br>\n",
    "> - `style_targets` is a list of tensors of size `len(style_layers)` containing the objects passed to the style loss function during the training process.\n",
    "> - `loss_fn_style` is the style loss function. <br>\n",
    "> - `loss_fn_content` is the content loss function. <br>\n",
    "<hr>\n",
    "</font>\n",
    "\n",
    "Note that we do not compute `content_targets` beforehand, given that the content will change depending on the image of the dataset. Thus, these targets must be computed within the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = ['r12','r22','r33','r43'] \n",
    "style_targets = ...\n",
    "loss_fn_style = ...\n",
    "\n",
    "content_layers = ['r22']\n",
    "loss_fn_content = ...\n",
    "\n",
    "loss_layers = style_layers + content_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat solutions/ex4.py\n",
    "# %load solutions/ex4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (optional)\n",
    "\n",
    "The training loop is a common training loop, in which we now optimize the **model** parameters (unlike the training loop above, in which we optimized the image). \n",
    "\n",
    "**If you have enough time, you can train your own model (~ 2'00 per epoch on Colab, and 0'40 per epoch on Kaggle), else you can skip this cell and use a pre-trained model in the following subsection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = utils.ImageTransformNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    agg_content_loss = 0\n",
    "    agg_style_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    pbar = tqdm(total=len(train_loader), desc=f\"Epoch {e+1}/{epochs}\", leave=False)\n",
    "    utils.print_head()\n",
    "    \n",
    "    for batch_id, (x, _) in enumerate(train_loader):\n",
    "        n_batch = len(x)\n",
    "        count += n_batch\n",
    "        \n",
    "        # Set gradients of out_img at zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = model(x)\n",
    "        \n",
    "        # `feature_a` is a list of tensors, corresponding to each feature maps indexed by loss_layers\n",
    "        features_y = vgg16(normalize_batch(y), loss_layers)\n",
    "        features_x = vgg16(normalize_batch(x), loss_layers)\n",
    "        \n",
    "        # Retrieve style_losses and content_losses\n",
    "        style_losses = []\n",
    "        content_losses = []\n",
    "        for feat_i, feature in enumerate(features_y):\n",
    "            if feat_i < len(style_layers):\n",
    "                gram_feat_i = style_targets[feat_i] # Get the batch of Gram matrices corresponding to this feature\n",
    "                style_losses.append(style_weight * loss_fn_style(feature, gram_feat_i[:n_batch, :, :]))\n",
    "            else: \n",
    "                content_losses.append(content_weight * loss_fn_content(feature, features_x[feat_i])) \n",
    "        \n",
    "        # The total loss is the sum of style and content losses\n",
    "        content_loss = sum(content_losses)\n",
    "        style_loss = sum(style_losses)\n",
    "        total_loss = content_loss + style_loss\n",
    "        \n",
    "        # Updates the model\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log\n",
    "        agg_content_loss += content_loss.item()\n",
    "        agg_style_loss += style_loss.item()\n",
    "\n",
    "        if (batch_id + 1) % 10 == 0:\n",
    "            avg_content_loss = agg_content_loss / (batch_id + 1)\n",
    "            avg_style_loss = agg_style_loss / (batch_id + 1)\n",
    "            avg_total_loss = avg_content_loss + avg_style_loss\n",
    "            utils.print_log(e, avg_content_loss, avg_style_loss, avg_total_loss, count, len(train_dataset))\n",
    "        \n",
    "        if (batch_id + 1) % 100 == 0:\n",
    "            model.eval().cpu()\n",
    "            save_model_filename = f\"models/epoch_{e+1}_checkpoint_{batch_id+1}.pth\"\n",
    "            torch.save(model.state_dict(), save_model_filename)\n",
    "            model.to(device).train()\n",
    "\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Save model\n",
    "    model.eval().cpu()\n",
    "    save_model_filename = \"models/epoch_\" + str(e+1) + \".pth\"\n",
    "    torch.save(model.state_dict(), save_model_filename)\n",
    "    model.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We now have a trained model, specialized in a specific style. We can now feed this model with any image we want, it has learnt to generate a new image in a specific style. Moreover, we can also feed a series of images (for example, an animation), and the model will transfer the style of this series of images, much faster than the first method.\n",
    "\n",
    "Your model has been saved in `models/` and you can find pre-trained models in `saved_models/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_filename = \"images/content-images/isae.jpg\"\n",
    "# model = \"saved_models/udnie.pth\"\n",
    "# model = \"saved_models/mosaic.pth\"\n",
    "model = \"models/epoch_10.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and transform the image\n",
    "content_image = Image.open(content_image_filename).convert('RGB')\n",
    "content_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "content_image = content_transform(content_image)\n",
    "content_image = content_image.unsqueeze(0).to(device)\n",
    "\n",
    "# Apply the style model\n",
    "with torch.no_grad():\n",
    "    style_model = utils.load_model(model).to(device)\n",
    "    output = style_model(content_image).cpu()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_image_johnson(output)\n",
    "plt.gcf().set_size_inches(7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_filename = \"images/gif/cat_typing.gif\"\n",
    "\n",
    "model = \"saved_models/udnie.pth\"\n",
    "# model = \"saved_models/mosaic.pth\"\n",
    "# model = \"models/epoch_10.pth\"\n",
    "\n",
    "SHOW_FRAMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the original gif\n",
    "gif = imageio.get_reader(gif_filename)\n",
    "n_frames = len(gif)\n",
    "\n",
    "# Load the style model\n",
    "style_model = utils.load_model(model).to(device)\n",
    "\n",
    "# Loop on each frame\n",
    "list_images_style = []\n",
    "list_images_original = []\n",
    "for i_frame, frame in enumerate(tqdm(gif, leave=False)):\n",
    "    content_image = utils.get_frame_gif(frame).to(device) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = style_model(content_image).cpu()[0]\n",
    "    \n",
    "    list_images_style.append(utils.get_image_johnson(output))\n",
    "    list_images_original.append(frame)\n",
    "\n",
    "    \n",
    "    if SHOW_FRAMES:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax1.imshow(frame)\n",
    "        utils.show_image_johnson(output, ax2)\n",
    "        print(f\"Frame {i_frame+1} / {n_frames}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anim = utils.make_gif_one_canvas(list_images_style)\n",
    "anim = utils.make_gif_both(list_images_original, list_images_style)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim.save(\"both_anim.gif\", writer=animation.PillowWriter(fps=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BF-FjXZEQVdq"
   },
   "outputs": [],
   "source": [
    "#@markdown Animation results\n",
    "display(HTML(utils.colab_video(\"./results/udnie_anim.mp4\")))\n",
    "display(HTML(utils.colab_video(\"./results/mosaic_anim.mp4\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the style model, the stylized animation can be more or less convincing. For instance, the `udnie.pth` model gives relatively correct results, while the model `mosaic.pth` can generate kind of \"flickering\" animation. The first style relies more on colors, while the second style is more about geometric patterns. Thus, given that the model transfers the style on each image independently of the other, geometric shapes that disappear and appear on each frame are much more visible and gives this lack of continuity. Some studies focus on this discontinuity in style transfer ([Ruder & al. (2016)](https://arxiv.org/pdf/1604.08610.pdf))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzl03Os4QVdp"
   },
   "source": [
    "<a class=\"anchor\" id=\"section3\"></a>\n",
    "## Going further \n",
    "\n",
    "### Improvements\n",
    "\n",
    "Neural Style transfer is a fairly broad research field: several studies then specializes in specific styles, focus on photorealistic outputs, extend the process to other images in 2D or 3D:\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://d3i71xaburhd42.cloudfront.net/b0760764dc573b519f76d5a79531d49af333c67a/3-Figure2-1.png\" width=\"100%\"><br>\n",
    "Taxonomy of Neural Style transfer techniques [source: <a href=https://arxiv.org/pdf/1705.04058.pdf> Jing & al (2018)</a>]\n",
    "</center>\n",
    "\n",
    "From a technical point of view, while Gatys's model implements convolutional networks, several later visual style transfer models rely on Generative Adversarial Networks (GAN). The main style transfer model using this approach is [CycleGAN](https://arxiv.org/pdf/1703.10593.pdf). It deals with a larger problem which is \"image-to-image translation\" that is to say transfering a concept from an image to another image. For instance, it manages to transform a zebra into a horse, or a summer picture into a winter picture, thus, style transfer is also a particular case of application of this model.\n",
    "\n",
    "### Style transfer in other artistic research fields\n",
    "\n",
    "Style transfer is also tackled in other artistic fields, especially in Music Information Retrieval (MIR), an active research field that includes computer science applied to music. \n",
    "\n",
    "\n",
    "Firstly, with an audio approach (i.e. in which the source material is the sound's waveform), the Google's Magenta team released the [DDSP](https://magenta.tensorflow.org/ddsp) (Differentiable Digital Signal Processing) model that can perform timbre transfer. In other words, you can have an instrument replacing another instrument in an audio track.\n",
    "From a technical point of view, the model is an autoencoder whose latent space is explainable, coupled with standard signal processing methods. Both the encoder and the decoder are trained on a specific timbre and thus specialized in a specific instrument. For instance, by combining a flute-encoder and a violin-decoder, you can transfer the timbre of the violin into a flute track (with some modifications in the actual paper...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BF-FjXZEQVdq"
   },
   "outputs": [],
   "source": [
    "#@markdown Demo DDSP: timbre transfer (taken from DDSP's website)\n",
    "print(\"Original violin track\")\n",
    "display(Audio(\"https://storage.googleapis.com/ddsp/timbre_transfer/instruments/violin_violin.mp3\"))\n",
    "print(\"Transfered with flute timbre\")\n",
    "display(Audio(\"https://storage.googleapis.com/ddsp/timbre_transfer/instruments/violin_flute2.mp3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiTZU2gDCJHT"
   },
   "source": [
    "Apart from the audio approach, with a symbolic approach (i.e. in which the source material is the sheet music), some models have also been implemented to process \"musical style transfer\" focusing on musical genre transfer. In other words, they manage to orchestrate or arrange a melody in the style of a specific composer. <br>\n",
    "MIR studies usually try to apply other domains model to music (for instance, a lot of [NLP methods](https://sites.google.com/view/nlp4musa-2021) are used for musical studies). In the case of style transfer, the [CycleGAN](https://arxiv.org/pdf/1809.07575.pdf) model have been implemented and adapted for musical results and gives preliminary results for later improvements in musicl style transfer based on GANs. <br>\n",
    "Symbolic style transfer is also tackled in the study [Groove2Groove](https://groove2groove.telecom-paris.fr/), which generates accompaniments of different styles. Technically, it relies on a content encoder and a style encoder, that feed a decoder based on recurrent networks.\n",
    "In the same way as Van Gogh's less convincing results, the results of Groove2Groove in the baroque style of Haendel are less satisfying. Indeed, the baroque style is usually characterized by a diverse musical texture and often includes more complex structures (like contrapuntal writing), contrary to pop music which is much more repetitive.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To sum up, Neural Style transfer aims at merging the style of an image into a content image. This can be done by optimizing the combined image itself, or by training a model on a specific style. In both cases, the idea is to compute the loss as a sum of, on the one hand, a content loss that compares abstractions of images, and on the other hand, a style loss that compares the co-occurrences of features.\n",
    "\n",
    "Neural Style Transfer goes beyond the simple examples seen in this notebook, by improving these techniques to other types of images, or by extending into other artistic research fields.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePlW_o0LQVdq"
   },
   "source": [
    "## Sources\n",
    "\n",
    "- [1] Gatys, L. A., Ecker, A. S., & Bethge, M. (2015). A neural algorithm of artistic style. _arXiv preprint arXiv:1508.06576._ [Link](https://arxiv.org/pdf/1508.06576.pdf) \n",
    "- [2] Gatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (pp. 2414-2423). [Link](https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\n",
    "- [3] Rhodes, N. (2021). CS 152 Day 15: Neural Networks/Deep Learning — Spring, 2021, _Harvey Mudd College, Computer Science_. [Link](https://www.youtube.com/playlist?list=PL2Yggtk_pK6-v87LproSNu1Mzz1oPd97X)\n",
    "- [4] Johnson, J., Alahi, A., & Fei-Fei, L. (2016, October). Perceptual losses for real-time style transfer and super-resolution. In _European conference on computer vision_ (pp. 694-711). Springer, Cham. [Link](https://arxiv.org/abs/1603.08155). Supplementary material: [Link](https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf)\n",
    "\n",
    "Adapted from these repositories :\n",
    "- Gatys & al. : [Pytorch Neural Style Transfer](https://github.com/leongatys/PytorchNeuralStyleTransfer)\n",
    "- Johnson & al. : [Pytorch Fast Neural Style](https://github.com/pytorch/examples/tree/master/fast_neural_style)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_style_transfer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2b144d8b56f64be28c385b7e011566d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49348527438e4cc5af9cd7355851ab92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "61bf4e11174040259c2d3827c1e5b49a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b144d8b56f64be28c385b7e011566d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f0337011b2e947689873747f8586b8b1",
      "value": "100%"
     }
    },
    "6a1dcb3bcc524cf0b24e202a7d62e506": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "737c53fecd7e4e4cb1d92691cd09c4f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1d0535e7f8b4e49890606f0ab9405da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_737c53fecd7e4e4cb1d92691cd09c4f4",
      "placeholder": "​",
      "style": "IPY_MODEL_6a1dcb3bcc524cf0b24e202a7d62e506",
      "value": " 548M/548M [00:06&lt;00:00, 69.9MB/s]"
     }
    },
    "a2ed53227b40453b81014b6d35654c14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_61bf4e11174040259c2d3827c1e5b49a",
       "IPY_MODEL_e6f37e53503f4050ba1fd4ccc4be2294",
       "IPY_MODEL_a1d0535e7f8b4e49890606f0ab9405da"
      ],
      "layout": "IPY_MODEL_b4319dea70944bc99fc4e0402199de6d"
     }
    },
    "b4319dea70944bc99fc4e0402199de6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df13b85061454cfca118216f50e65b8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6f37e53503f4050ba1fd4ccc4be2294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df13b85061454cfca118216f50e65b8f",
      "max": 574673361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49348527438e4cc5af9cd7355851ab92",
      "value": 574673361
     }
    },
    "f0337011b2e947689873747f8586b8b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
